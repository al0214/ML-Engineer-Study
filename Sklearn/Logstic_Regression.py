# -*- coding: utf-8 -*-
"""11. 로지스틱_회귀.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EF-9vP773XrjridL_MedNsVuQpPgbGIs

<h2>데이터 준비하기</h2>
판다스의 read_csv() 함수로 CSV 파일을 데이터 프레임으로 변환한 다음 head() 메서드로 출력해 보겠습니다.
"""

import pandas as pd
fish = pd.read_csv("https://bit.ly/fish_csv_data")
fish.head()

"""그럼 어떤 종류의 생선이 있는지 Species 열에서 고유한 값을 추출해 보겠습니다
<br>
판다스의 unique() 함수를 사용하면 간단합니다.
"""

print(pd.unique(fish['Species']))

fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()

print(fish_input[:5])

fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

"""<h2>k-최근접 이웃 분류기의 확률 예측</h2>
KNeighborsClassifier 클래스 객체를 만들고 훈련 세트로 모델을 훈련한 다름 훈련 세트와 테스트 세트의 점수를 확인해 보겠습니다.
"""

from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

"""앞서 fish 데이터프레임에서 7개의 생선이 있었던 것을 기억하나요? 
<br>
타깃 데이터를 만들 때 fish['Species']를 사용해 만들었기 때문에 훈련 세트와 테스트 세트의 타깃 데이터에도 7개의 생선 종류가 들어 있습니다.
<br>
타깃 데이터에 2개 이상의 클래스가 포함된 문제를 다중 분류(multi-class-classification)라고 부릅니다.
<br><br>
타깃값을 그대로 사이킷런 모델에 전달하면 순서가 자동으로 알파벳 순으로 매겨집니다.<br>
따라서 pd.unique(fish['Species'])로 출력했던 순서와 다릅니다.<br>
KNeighborsClassifier에서 정렬된 타깃값은 classes_ 속성에 저장되어 있습니다.
"""

print(kn.classes_)

"""predict() 메서드는 친절하게도 타깃값으로 에측을 출력합니다. 테스트 세트에 있는 처음 5개 샘플의 타깃값을 예측해 보겠습니다."""

print(kn.predict(test_scaled[:5]))

"""이 5개 샘플에 대한 예측은 어떤 확률로 만들어졌을까요? 사이킷런의 분류 모델은 predict_proba() 메서드로 클래스별 확률값을 반환합니다. <br> 테스트 세트에 있는 처음 5개의 샘플에 대한 확률을 출력해 보죠. 넘파이 round() 함수는 기본으로 소수점 첫쨰 자리에서 반올림을 하는데, <br> decimals 매개변수로 유지할 소수점 아래 자릿수를 지정할 수 있습니다"""

import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4)) # 소수점 네 번째 자리까지 표기합니다. 다섯 번째 자리에서 반올림합니다.

"""predict_proba() 메서드의 출력 순서는 앞서 보았던 classes_ 속성과 같습니다. """

distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])

"""<h2>로지스틱 회귀</h2>
로지스틱 회귀는 이름은 회귀이지만 분류 모델입니다. 이 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습합니다. <br>예를 들면 다음과 같습니다.
<br>
<h4>z = a X (Weight) + b X (Length) + c X (Diagonal) + d X (Height) + e X (Width) + f</h4>
<br>
여기에서 a, b, c, d, e는 가중치 혹은 계수 입니다. z는 어떤 값도 가능합니다. 하지만 확률이 되려면 0~1 (또는 0~100%) 사이 값이 되어야 합니다. z가 아주 큰 음수일 때 0이 되고, z가 아주 큰 양수일 때 1이 되도록 바꾸는 방법이 없을까요?<br>
<h3>시그모이드 함수(sigmoid) 또는 로지스틱 함수(logistic function)를 사용하면 가능합니다
<img src="https://drive.google.com/uc?id=1nQ5YgHDjhNvZ2kGV9maZAG27pSs8Zh_C" height = 200 width= 700>
<br>

선형 방정식의 출력 z의 음수를 사용해 자연 상수 e를 거듭제곱하고 1을 더한 값의 역수를 취합니다
<br>
이 수식을 사용하면 오른쪽 그래프를 만들수 있습니다.
"""

import numpy as np
import matplotlib.pyplot as plt
z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z)) # Numpy의 numpy.exp() 함수는 밑이 자연상수 e인 지수함수(e^x)로 변환해줍니다.
plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()

"""시그모이드 함수의 출력은 정말 0에서 1까지 변환하는군요. 그럼 로지스틱 회귀 모델을 훈련해 보죠
<br>
사이킷런에는 로지스틱 회귀 모델인 LogisticRegression 클래스가 준비되어 있습니다.
<br>
훈련하기 전에 간단한 이진 분류를 수행해 보겠습니다. 
<br>
이진 분류일 경우 시그모이드 함수의 출력이 0.5보다 크면 양성 클래서, 0.5보다 작으면 음성 클래스로 판단합니다.

<h3>로지스틱 회귀로 이진 분루 수행하기</h3>
넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있습니다. 이를 볼리언 인덕싱(boolean indexing)이라고 합니다.
"""

char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

"""bream_smelt_indexes 배열은 도미와 빙어일 경우 True이고 그 외는 False 값이 들어가 있습니다.<br>
따라서 이 배열을 사용해 train_scaled와 train_target 배열에 볼리언 인덱싱을 적용하면
<br> 
손쉽게 도미와 빙어 데이터만 골라낼 수 있습니다.
<br>
<h4>LogisticRegression 클래스는 선형 모델이므로 sklearn.linear_model 패키지 아래 있습니다.</h4>
"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

# 훈련한 모델을 사용해 train_bream_smelt에 있는 처음 5개 샘플을 에측해 보죠.
print(lr.predict(train_bream_smelt[:5]))

"""두 번째 샘플을 제외하고는 모두 도미로 예측했습니다. 예측 확률은 predict_proba() 메서드에서 제공합니다."""

print(lr.predict_proba(train_bream_smelt[:5]))

"""첫 번째 열이 음성 클래스(0)에 대한 확률이고 두번째 열이 양성 클래스(1)에 대한 확률입니다. <br>
사이킷런은 타깃값을 알파벳순으로 정렬하여 사용합니다. classes_ 속성에서 확인해 보죠.
"""

print(lr.classes_)

"""로지스틱 회귀로 성공적인 이진 분류를 수행했군요! 그럼 선형 회귀에서처럼 로지스틱 회귀가 학습한 계수를 확인해 보죠"""

print(lr.coef_, lr.intercept_)

"""따라서 이 로지스틱 회귀 모델이 학습한 방정식은 다음과 같습니다.<br>
z = -0.404 x (Weight) - 0.576 x (length) - 0.663 x (Diagonal) - 1.013 x (Height) - 0.732 x (Width) - 2.161<br>
LogisticRegression 모델로 z 값을 계산해 볼 수 있을까요? 
LogisticRegression 클래스는 decision_function() 메서드로 z 값을 출력할 수 있습니다.
train_bream_smelt의 처음 5개 샘플의 z 값을 출력해 보죠.
"""

decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

from scipy.special import expit
print(expit(decisions))

"""<h3>로지스틱 회귀로 다중 분류 수행하기</h3>
LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용합니다. max_iter 매개변수에서 반복 횟수를 지정하며 기본값은 100입니다.
<br>
여기에 준비한 데이터셋을 사용해 모델을 훈련하면 횟수가 부족하다는 경고가 발생합니다. 충분하게 훈련시키기 위해 반복 횟수를 1,000으로 늘리겠습니다.
<br>
LogisicRegression은 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제합니다. 이런 규제를 L2 규제라고도 부릅니다. 릿지 회귀에서는 alpha 매개변수로
<br> 규제의 양을 조절했습니다. alph가 커지면 규제도 커집니다. LogisticRegression에서 규제를 제어하는 매개변수는 C입니다. 하지만 C는 alpha와 반대로 
<br>
작을수록 규제가 커집니다.C의 기본값은 1 입니다. 여기에서는 규제를 조금 완화하기 위해 20으로 늘리겠습니다.
"""

lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

# 훈련 세트와 테스트 세트에 대한 점수가 높고 과대적합이나 과소적합으로 치우친 것 같지 않습니다.
# 좋네요. 그럼 테스트 세트의 처음 5개 샘플에 대한 예측을 출력해 보죠.

print(lr.predict(test_scaled[:5]))

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

# 클래스 정보를 확인해 보죠
print(lr.classes_)

# 다중 분류일 경우 선형 방정식은 어떤 모습일까요? coef_와 intercept_의 크기를 출력해 보겠습니다.
print(lr.coef_.shape, lr.intercept_.shape)

"""이 데이터는 5개의 특성을 사용하므로 coef_ 배열의 열은 5개 입니다. 그런데 행이 7이군요. 다중 분류는 클래스마다 z 값을 하나씩 계산합니다. 다중 분류는 이와 달리 소프트맥스 함수를 사용하여 7개의 z 값을 확률로 변환합니다.
<br>
<h3>소프트맥스 함수가 뭔가요?</h3>

시그모이드 함수는 하나의 선형 방정식 출력값을 0 ~ 1 사이로 압축합니다. 이와 달리 소포트맥스 함수는 여러 개의 선형 방정식의 출력값을 0 ~ 1 사이로 압축하고 전체 합이 1이 되도록 만듭니다. 이를 위해 지수 함수를 사용하기 때문에 정규화된 지수 함수 라고도 부릅니다.
<br>

<h3>소프트맥스 함수 계산 방식</h3>
먼저 7개의 z 값의 이름을 z1에서 z7이라고 붙이겠습니다. z1~z7까지 값을 사용해 지수 함수 e^z1~e^z7을 계산해 모두 더합니다. 이를 e_sumㅇ이라고 하겠습니다.
<br>
<h4>e_sum = e^z1 + e^z2 + e^z3 + e^z4 + e^z5 + e^z6 + e^z7</h4>
그다음 e^z1~e^z7을 각가 e_sum으로 나누어 주면 됩니다. 
<br>
<h4>s1 = e^z1/e_sum, s2 = e^z2/e_sum, 𑇐𑇐𑇐, s7 = e^z7/e_sum</h4>
s1에서 s7까지 모두 더하면 분자와 문모가 같아지므로 1이 됩니다. 7개의 생성에 대한 확률의 합은 1이 되어야 하므로 잘 맞네요
 
"""

# decision_function() 메서드로 z1~z7까지의 값을 구한다음 소프트맥스 함수를 사용해 확률로 바꾸어 보겠습니다.
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))

from scipy.special import softmax
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))

"""softmax()의 axis 매개변수는 소프트맥스를 계산할 축을 지정합니다. 여기에서는 axis=1로 지정하여 각 행, 즉 각 샘플에 대한 소프트맥스를 계산합니다. 만약 axis 매개변수를 지정하지 않으면 배열 전체에 대해 소프트맥스를 계산합니다."""

