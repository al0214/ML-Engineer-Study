{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166f5192",
   "metadata": {},
   "source": [
    "<h3>점진적 학습</h3>\n",
    "훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련할 수 없을까요?<br>\n",
    "이렇게 할 수 있다면 훈련에 사용한 데이터를 모두 유지할 필요도 없을 것 입니다.<br>\n",
    "<br>\n",
    "이런 식의 훈련 방식을 점진적 학습 또는 온라인 학습이라고 부릅니다.<br>\n",
    "대표적인 알고리즘은 확률적 경사 하강법입니다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>확률적 경사 하강법</h3>\n",
    "확률적 경사 하강법에서 확률적이란 말은 '무작위하게' 혹은 '랜덤하게'의 기술적인 표현입니다. 경사는 기울기를 뜻합니다.<br>\n",
    "다시 말해 경사 하강법은 경사를 따라 내려가는 방법을 말합니다.<br>\n",
    "등산을 예를 들어 가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 것이 중요합니다.<br> \n",
    "이렇게 내려오는 과정이 바로 경사 하강법 모델을 훈련하는 것 입니다.<br>\n",
    "전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾습니다!<br>\n",
    "이처럼 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것이 확률적 경사 하강법입니다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>SGD (Stochastic gradient descent) </h3>\n",
    "SGD는 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려갑니다.<br>\n",
    "이런 식으로 전체 샘플을 모두 사용할 때까지 반복합니다.<br>\n",
    "이때 모든 샘플을 사용했을때도 다 내려오지 못하였을면 다시 처음부터 시작합니다.<br>\n",
    "이렇게 만족할만한 위치에 도달할 때까지 계속 내려가면 됩니다.<br>\n",
    "확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 에포크라고 부릅니다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>미니배치 경사 하강법</h3>\n",
    "SGD에서 훈련 세트의 샘플을 하나씩 사용하는 것이 걱정된다면 여려개를 사용해보는 것이 어떨까요?<br>\n",
    "여러개의 샘플을 사용해 경사 하강법을 수행하는 방식을 미니배치 경사 하강법이라고 부릅니다<br>\n",
    "<br>\n",
    "\n",
    "<h3>배치 경사 하강법</h3>\n",
    "극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용하는 방법입니다.<br>\n",
    "전체 데이터를 사용하기 때문에 가장 안전한 방법이지만 그만큼 컴퓨터 자원을 많이 사용하며 <br>\n",
    "데이터가 너무 많을때 전체 데이터를 모두 읽을 수 없을지도 모른다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>손실 함수</h3>\n",
    "어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준입니다.<br>\n",
    "그렇다면 손실 함수의 값은 작을수록 좋습니다.<br>\n",
    "하지만 어떤 값이 최솟값을인지는 알지 못합니다.<br>\n",
    "가능한 많이 찾아보고 만족할만한 수준이라면 산을 다 내려왔다고 인정해야 합니다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>손실 함수와 비용 함수</h3>\n",
    "손실 함수는 훈련 세트에서 한 샘플에 대한 손실을 나타냅니다.<br>\n",
    "비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말합니다.<br>\n",
    "보통 이 둘은 엄격히 구분하지 않고 섞어서 사용합니다.<br>\n",
    "<br>\n",
    "\n",
    "<h3>로지스틱 손실 함수</h3>\n",
    "양성 클래스(타깃 = 1)일 떄 손실은 -log(예측 확률)로 계산합니다. 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 됩니다.<br>\n",
    "음성 클래스(타깃 = 0)일 때 손실은 -log(1-예측 확률)로 계산합니다. 이 예측 확률이 0에서 멀어질수록 손시른 아주 큰 양수가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7416f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov  4 2022, 11:11:31) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "485e7cfbc8d8237bc6e9bb79a59cc0c32fb2f4ed28c6621f2276f91bdad43e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
