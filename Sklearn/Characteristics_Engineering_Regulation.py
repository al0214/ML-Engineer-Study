# -*- coding: utf-8 -*-
"""10. 특성 공학과 규제.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DopF9ES_HuNWjVu1cJ67MdKRFkCpkoGb
"""

import pandas  as pd
# 데이터 프레임 ( 넘파이 배열과 다르게 타입이 다른 데이터를 섞어서 가져올 수 있다.)
df = pd.read_csv('https://bit.ly/perch_csv')
print(f'데이터 프레임 배열 \n{df}\n')

perch_full = df.to_numpy()
print(f'넘파이 배열 \n{perch_full}')

import numpy as np
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)

"""<h2>사이킷런의 변환기</h2>
사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공합니다.
<br>
사이킷언에서는 이런 클래스를 변환기(transformer)라고 부릅니다.
<br>
사이킷런의 모델 클래스에 일관된 fit(), score(), predict() 메서드가 있는 것처럼
<br>
변환기 클래스는 모두 fit(), transform() 메서드를 제공합니다.
<br>
PolynomialFeatures 클래스는 sklearn.preprocessing 패키지에 포합되어 있습니다.
"""

# 훈련(fit)을 해야 변환(transform)이 가능합니다. 그 이유는 사이킷런의 일관된 api 때문에 두 단계로 나누어져 있기 때문입니다.
# 이 불편함을 해결하기 위해서 fit_transform 메서드도 있습니다.
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([[2,3]])

print(poly.transform([[2,3]]))

"""PolynomialFeaturess 클래스는 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가합니다
<br>
2와 3을 각기 제곱한 4와 9가 추가되었고, 2와 3을 곱한 6이 추가 되었습니다. 근데 왜 1이 추가되었을까요?
<br>
무게 = a X 길이 + b x 높이 + c x 두께 + d x 1
<br>
선형 방정식의 절편을 항상 값이 1인 특성과 곱해지는 계수라고 볼 수 있습니다.
<br>
하지만 사이킷런의 선형 모델은 자동으로 절편을 추가 하므로 굳이 특성을 만들 필요가 없습니다.
<br>
incloud_bias = False로 지정하여 다시 특성을 변환하겠습니다
"""

poly = PolynomialFeatures(include_bias=False)
poly.fit_transform([[2, 3]])

poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)

"""9개의 특성이 어떻게 만들어졌는지 확인하는 아주 좋은 방법을 제공합니다.
<br>
get_feature_names() 메서드를 호출하면 9개의 특성이 각각 어떤 입려의 조합으로 만들어졌는지 알려줍니다.
"""

poly.get_feature_names()

test_poly = poly.transform(test_input)

"""<h2>다중 회귀 모델 훈련하기</h2>
다중 회귀 모델을 훈련하는 것은 선형 회귀 모델을 훈련하는 것과 같습니다.
<br>
다만 여러 개의 특성을 사용하여 선형 회귀를 수행하는 것 뿐이죠.
<br>
LinearRegression 클래스를 임포트하고 앞에서 만든 train_poly를 사용해 모델을 훈련시켜 보겠습니다.
"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))

"""특성을 더 많이 추가하면 어떻게 될까요?
<br>
PolynomialFeatures 클래스의 degree 매개변수를 사용하여 필요한 고차항의 최대 차수를 지정할 수 있습니다
"""

poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape)

"""만들어진 특성의 개수가 무려 55개나 됩니다. train_poly 배열의 열의 개수가 특성의 개수입니다.
<br>
이 데이터를 사용해 선형 회귀 모델을 다시 훈련하겠습니다.
"""

lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))

"""0.9999999999991097 거의 완벽한 점수입니다. 테스트 세트에 대한 점수는 어떨까요?"""

print(lr.score(test_poly, test_target))

"""음수가 나왔습니다. 이게 무슨 일이죠?
<br>
특성의 개수를 크게 늘리면 선형 모델은 아주 강력해 집니다. 훈련 세트에 대해 거의 완벽하게 학습할 수 있죠. 하지만 이런 모델은 훈련 세트에 너무 과대적합되므로 테스트 세트에서는 형편없는 점수를 만듭니다.

이 문제를 해결하기 위해서는 다시 특성을 줄여야 합니다.
<br>
<h2>규제</h2>
규제는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것을 말합니다.
<br>
즉 모델이 훈련 세트에 과대적합되지 않도록 만드는 것이죠. 선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일입니다.
<img src="https://drive.google.com/uc?id=19Uh5e63fE1f5yQ640ki6pRWX30YPPOsx" height=300 width=6000>
<br>
<br>
위 사진의 왼쪽은 훈련 세트를 과도하게 학습했고 오른쪽 기울기를 줄여 보다 보편적인 패턴을 학습하고 있습니다.
<br>
앞서 55개의 특성으로 훈련한 선형 회귀 모델의 계수를 규제하여 훈련 세트이 점수를 낮추고 대신 테스트 세트의 점수를 높여 보겠습니다.
<br>
일반적으로 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 많이 다르면 공정하게 제어되지 않을 겁니다. 그러면 규제를 적용하기 전에 먼저 정규화를 해야겠군요
<br>
하지만 이번에는 사이킷런에서 제공하는 StandardScaler 클래스를 사용하겠습니다.
"""

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

"""먼저 StandardScaler 클래스의 객체 ss를 초기화한 후 PolynomialFeatures 클래스로 만든 train_poly를 사용해 이 객체를 훈련합니다. 여기에서도 다시 한번 강조하지만 꼭 훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야 합니다.
<br>
이제 표준 점수로 변환한 train_poly와 test_poly가 준비되었습니다
"""

# 훈련 세트에서 학습한 평균과 표준편차는 StandardScaler 클래스 객체의 mean_, scale_ 속성에 저징됩니다.
# 특성마다 계산하므로 55개의 평균과 표준 편차가 들어 있습니다.

"""<h2>선형 회귀 모델에 규제를 추가한 모델을 릿지(ridge)와 라쏘(lasso)라고 부릅니다.</h2>

<h2>릿지 회귀</h2>
릿지 회귀는 계수를 제곱한 값을 기준으로 규제를 적용합니다.
릿지는 sklearn.linear_model 패키지 안에 있습니다. 사이킷런 모델을 사용할때 편리한 점은 훈련하고 사용하는 방법이 항상 같다는 것 입니다. 모델 객체를 만들고 fit() 메서드에서 훈련한 다음 score() 메서드로 평가합니다.
"""

from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print('\n션형 회귀에서는 거의 완벽에 가까웠던 점수가 조금 낮아졌군요. 이번에는 테스트 세트에 대한 점수를 확인하겠습니다')

print(ridge.score(test_scaled, test_target))

"""테스트 세트 점수가 정상으로 돌아왔습니다. 확실히 많은 특성을 사용했음에도 불구하고 훈련 세트에 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능을 내고 있습니다.
<br>
<br>
릿지와 라쏘 모델을 사용할 떄 규제의 양을 임의로 조절할 수 있습니다. 모델 객체를 만들 떄 alpha 매개변수로 규제의 강도를 조절합니다. alpha 값이 크면 규제 강도가 세지므로 계수 값을 줄이고 조금 더 과소적합되도록 유도합니다.

<h2>Note</h2>
alpha 값은 릿지 모델이 학습하는 값이 아니라 사전에 우리가 지정해야 하는 값입니다. 이렇게 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를
<br>
Hyperparameter라고 부릅니다. 사이킷런과 같은 머신러닝 라이브러리에서 Hyperparameter는 클래스와 메서드의 매개변수로 표현됩니다.

<h3>적절한 규제 강도 조절하기</h3>
적절한 alpha 값을 찾는 방법은 alpha 값에 대한 R^2 값의 그래프를 그려 보는 것 입니다. 
<br>
훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha 값이 됩니다.
"""

# 먼저 alpha 값을 바꿀 때마다 score() 메서드의 결과를 저장할 리스트를 만듭니다.
import matplotlib.pyplot as plt
train_score = []
test_score = []

# alpha 값을 0.001부터 100까지 10배씩 늘려가며 릿지 모델을 훈련한다음 훈현 세트와 테스트 세트의 점수를 리스트에 저장합니다.
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
  # 릿지 모델 생성
  ridge = Ridge(alpha = alpha)
  # 릿지 모델 훈련
  ridge.fit(train_scaled, train_target)
  # 훈련 세트와 테스트 점수를 저장
  train_score.append(ridge.score(train_scaled, train_target))
  test_score.append(ridge.score(test_scaled, test_target))

# alpha 값을 0.001부터 10배씩 늘렸기 때문에 이대로 그래프를 그리면 그래프 왼쪽이 너무 촘촘해집니다.
# 이를 해결하기위해 로그 함수로 바꾸어 지수를 표현하겠습니다. (로그 함수란 0.001일때 -3으로 변경하는 함수이다.)

plt.plot(np.log10(alpha_list), train_score, label='train')
plt.plot(np.log10(alpha_list), test_score, label='test')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()
plt.show()

"""이 그래프의 왼쪽을 보면 훈련 세트와 테스트 세트의 점수 차이가 아주 큽니다. 훈련 세트에는 잘 맞고 테스트 세트에는 형편없는 과대적합의 전형적인 모습이죠. 
<br><br>
적절한 alpha 값은 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 -1, 즉 10^-1=0.1 입니다.
"""

ridge = Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(f'훈련 세트 점수 : {ridge.score(train_scaled, train_target)}%')
print(f'테스트 세트 점수 : {ridge.score(test_scaled, test_target)}%')

"""<h2>라쏘 회귀</h2>
계수의 절댓값을 기준으로 규제를 적용합니다. 라쏘 모델은 훈련하는 것은 릿지와 매우 비슷합니다. Ridge 클래스를 Lasso 클래스로 바꾸는 것이 전부입니다.
"""

from sklearn.linear_model import Lasso 
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(f'훈련 세트 : {lasso.score(train_scaled, train_target)}%')
print(f'테스트 세트 : {lasso.score(test_scaled, test_target)}%')

"""라쏘 모델도 alpha 매개변수로 규제의 강도를 조절할 수 있습니다."""

train_score = []
test_score = []
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
  # 라쏘 모델을 만듭니다.
  lasso = Lasso(alpha=alpha, max_iter=10000)
  # 라쏘 모델을 훈련합니다.
  lasso.fit(train_scaled, train_target)
  # 훈련 점수와 테스트 점수를 저장합니다.
  train_score.append(lasso.score(train_scaled, train_target))
  test_score.append(lasso.score(test_scaled, test_target))

"""라쏘 모델을 훈련할 때 ConvergenceWarning이란 경고가 발생 할 수 있습니다. 사이킷런의 라쏘 모델은 최적의 계수를 찾기위해 반복적인 계산을 수행하는데,<br> 지정한 반복 횟수가 부족할 때 이런 경고가 발생합니다.
이러한 문제를 해결하기위해 max_iter 매개변수의 값을 10000으로 지정했습니다.
"""

plt.plot(np.log10(alpha_list), train_score, label="train")
plt.plot(np.log10(alpha_list), test_score, label="test")
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend(loc="lower left")
plt.show()

"""위 그래프의 왼쪽은 과대적합을 보여주고 있고, 오른쪼긍로 갈수록 훈련 세트와 테스트 세트의 점수가 좁혀지고 있습니다. 가장 오른쪽은 아주 크게 점수가 떨어집니다. 이 지점은 분명 과소적합되는 모델일 것 입니다.
라쏘 모델에서 최적의 alpha 값은 10^1=10 입니다. 이 값으로 다시 모델을 훈련하겠습니다.
"""

lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)
print(f'훈련 세트 : {lasso.score(train_scaled, train_target)}%')
print(f'테스트 세트 : {lasso.score(test_scaled, test_target)}%')

"""모델이 잘 훈련된 것 같군요. 특성을 많이 사용했지만, 릿지와 마찬가지로 모델이 과대적합을 잘 억제하고 테스트 세트의 성능을 크게 높였습니다.
앞에서 라쏘 모델은 계수의 값을 아예 0으로 만들 수 있다고 했던것을 기억하나요?
라쏘 모델의 계수는 coef_ 속성에 저장되어 있습니다. 이중에 0인것을 헤아려 보겠습니다.
"""

print(np.sum(lasso.coef_ == 0))

"""정말 많은 계수가 0이 되었군요. 55개의 특성을 모델에 주입했지만 라쏘 모델이 사용한 특성은 15개 밖에 되지 않습니다. 이런 특징 떄문에 라쏘 모델을 유용한 특성을 골라내는 용도로 사용할 수 있습니다."""